# Spotify playlist generator based on perceived facial emotion

### Screenshots of finished webapp
<p align="center">
<img src="https://raw.githubusercontent.com/Ollie-Boyd/spotify-facial-emotion/master/screenshots/Screen_Recording_2020_08_25_at_17_42_46.gif" width=50% height=auto%>
  <img src="https://raw.githubusercontent.com/Ollie-Boyd/spotify-facial-emotion/master/screenshots/Screen_Recording_2020_08_25_at_17_42_46%20(1).gif" width=50% height=auto%>
 <img src="https://raw.githubusercontent.com/Ollie-Boyd/spotify-facial-emotion/master/screenshots/spicify_login.png" width=80% height=auto%>
  <img src="https://raw.githubusercontent.com/Ollie-Boyd/spotify-facial-emotion/master/screenshots/cam_view.png" width=80% height=auto%>
 <img src="https://raw.githubusercontent.com/Ollie-Boyd/spotify-facial-emotion/master/screenshots/playlist_view.png" width=80% height=auto%>
</p>

## Brief
As our cohort was pretty strong we had an open brief to choose our tech stack and brief.
5 was the biggest group I'd worked with on a project however it only took an hour or so to go from absolutely no ideas to the idea of combining facial emotional recognition with the Spotify API (which contains emotional traits in the song API) and making a playlist generator based on a user's existing playlists. 

## Planning
We made database relationship diagrams, 5 sets of wireframes (one per person then cherry-picked features we liked), and mapped out the React components. 
We split up the reasearch, I researched various facial-recognition libraries and APIs and a bit on accessing the webcam. I opted for the Azure API as the emotion API response looked 

 <img src="
https://raw.githubusercontent.com/Ollie-Boyd/spotify-facial-emotion/master/screenshots/api.png" width=80% height=auto%>



## To run

### /frontend/

```
npm install
```
```
npm run serve
```

### /backend_node/

```
npm install
```
```
npm run server:dev
```
